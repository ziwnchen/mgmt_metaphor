{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5abadfbc",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline\n",
    "- select congressional speech sentences that contain certain objects\n",
    "- setences include human or nonhuman objects, but does not necessarily contain management expression\n",
    "- similar preprocessing pipeline should be applied to other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e607200-e98e-4772-b984-4721f892e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from multiprocessing import Pool, cpu_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b79ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "english_words = set(words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be0d64",
   "metadata": {},
   "source": [
    "## Convert individual file to CSV\n",
    "- copy from txt_to_csv ipynb\n",
    "- note that we did not use those filtering criteria to select sentences containing objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4be498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_dataframe(filepath):\n",
    "    # Initialize an empty list to store the rows\n",
    "    rows = []\n",
    "    \n",
    "    # Open the file, ignoring decoding errors\n",
    "    with open(filepath, 'r', errors='ignore') as file:\n",
    "        for line in file:\n",
    "            # Split each line by '|' and strip the newline character\n",
    "            parts = line.strip().split('|')\n",
    "            # Append the split line to the rows list\n",
    "            if len(parts) == 2 and parts[0]!='speech_id':  # Ensure the line has two parts\n",
    "                rows.append(parts)\n",
    "    \n",
    "    # Convert the list of rows to a DataFrame\n",
    "    df = pd.DataFrame(rows, columns=['speech_id', 'speech'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf55373",
   "metadata": {},
   "source": [
    "# Initital Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5779b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sentences containing given objects, this would be a coarse filtering\n",
    "# there might be chances that a given word is not in the sentence (eg, want parent but only get apparent)\n",
    "def sentence_extract(text, object_ls):\n",
    "    relevant_sents = []\n",
    "    if isinstance(text, float) == False:\n",
    "        sents = nltk.sent_tokenize(text)\n",
    "        for sent in sents:\n",
    "            sent_lower = sent.lower()\n",
    "            for word in object_ls:\n",
    "                if word in sent_lower:\n",
    "                    sent = re.sub(' +', ' ', sent) #remove more than one spaces\n",
    "                    relevant_sents.append((word,sent))\n",
    "    return relevant_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24943a",
   "metadata": {},
   "source": [
    "# Further Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf3f6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def lemmatize_words(words):\n",
    "    # Create a dictionary to store the results\n",
    "    lemmas = {}\n",
    "    # Process each word using spaCy to obtain its lemma\n",
    "    for word in words:\n",
    "        # Convert the word into a spaCy document object to access linguistic annotations\n",
    "        doc = nlp(word)\n",
    "        for token in doc:\n",
    "            # Add the original word and its lemma to the dictionary\n",
    "            lemmas[word] = token.lemma_\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4547fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_token_in_text(doc, token_id):\n",
    "    masked_tokens = []\n",
    "    for token in doc:\n",
    "        if token.i == token_id:\n",
    "            masked_tokens.append('[MASK]')\n",
    "        else:\n",
    "            masked_tokens.append(token.text)\n",
    "\n",
    "    # Join the tokens back to form the masked sentence\n",
    "    masked_text = ' '.join(masked_tokens)\n",
    "    return masked_text\n",
    "\n",
    "# Further filtering: remove sentences that did not contain the word we need\n",
    "# also make sure that the word detected is a noun\n",
    "def sentence_structure_check(word, text, lemma_dict):\n",
    "    # check lemma form\n",
    "    if_contain_lemma = False\n",
    "    word_lemma = lemma_dict[word] \n",
    "    head_verb = \"NA\"\n",
    "    if_VO=False\n",
    "    if_SV=False\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    for i in range(len(doc)):\n",
    "        token = doc[i]\n",
    "        if token.lemma_ == word_lemma:\n",
    "            if token.pos_ == \"NOUN\": # only select nouns\n",
    "                if_contain_lemma= True\n",
    "                focal_object = token.lemma_\n",
    "                focal_object_id = token.i  # ID of the object\n",
    "                # check structure: VO\n",
    "                if token.dep_ in ['dobj', 'pobj', 'iobj'] and token.head.pos_ == \"VERB\":\n",
    "                    if_VO=True\n",
    "                    head_verb = token.head.lemma_\n",
    "                # check structure: SV\n",
    "                if 'subj' in token.dep_ and token.head.pos_ == \"VERB\":\n",
    "                    if_SV=True\n",
    "                    head_verb = token.head.lemma_\n",
    "                break\n",
    "                \n",
    "    if if_contain_lemma==True:\n",
    "        object_masked_sent = mask_token_in_text(doc, focal_object_id)\n",
    "        return True, (focal_object, focal_object_id, object_masked_sent), (if_VO, if_SV, head_verb)\n",
    "    else:\n",
    "        return False, (\"NA\", \"NA\", \"NA\"), (if_VO, if_SV, head_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee51f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function to deal with list of (word, sent) structure\n",
    "def apply_parallel(df_group):\n",
    "    df_group['result'] = df_group['relevant_sent'].apply(lambda x:  sentence_structure_check(x[0],x[1], total_lemma_dict))\n",
    "    return df_group\n",
    "\n",
    "def parallelize_dataframe(df, func, n_chunks, ncores):\n",
    "    pool = Pool(ncores)\n",
    "\n",
    "    df_split = np.array_split(df, n_chunks)\n",
    "    results = []\n",
    "\n",
    "    with tqdm(total=len(df_split)) as pbar:\n",
    "        for result in pool.imap_unordered(func, df_split):\n",
    "            results.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9feff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample sentences that have at least 10 tokens\n",
    "def sent_length(sent):\n",
    "    return len(sent.split(\" \"))\n",
    "\n",
    "# check ocr quality\n",
    "def ocr_quality_check(text):\n",
    "    tokens = text.split(\" \")\n",
    "    non_english_count = sum(1 for token in tokens if token not in english_words)\n",
    "    total_words = len(tokens)\n",
    "    if total_words == 0:\n",
    "        return 1\n",
    "    non_english_ratio = non_english_count / total_words\n",
    "    return non_english_ratio\n",
    "\n",
    "# remove special tokens/punctuations except for comma, period, and question mark\n",
    "def remove_special_tokens(text):\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_formatting(df):\n",
    "    df['if_selected'] = df['result'].apply(lambda x: x[0])\n",
    "    df['object'] = df['result'].apply(lambda x: x[1][0])\n",
    "    df['object_mask'] = df['result'].apply(lambda x: x[1][2])\n",
    "    df['sent_unmask'] = df['relevant_sent'].apply(lambda x: x[1])\n",
    "    df['if_vo'] = df['result'].apply(lambda x: x[2][0])\n",
    "    df['if_sv'] = df['result'].apply(lambda x: x[2][1])\n",
    "    df['head_verb'] = df['result'].apply(lambda x: x[2][2])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2faea",
   "metadata": {},
   "source": [
    "# Running with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324167c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/zfs/projects/faculty/amirgo-management/congress/speeches/\"\n",
    "processed_path = \"/zfs/projects/faculty/amirgo-management/congress/speeches_processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be59959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to match words in sentences, may not be full because of potential plural forms\n",
    "mind_list=['anger','stress','pain', 'emotion','expectation','anxiety','anxieties','trust','feeling','grief',\n",
    "           'happiness', 'sadness', 'fear', 'disgust', 'surprise', 'shame', 'guilt','love','joy', \n",
    "           'despair','disappointment','excitement']\n",
    "body_list=['weight','health','care','disease','illness','diabetes',',medication','nutrition','addiction']\n",
    "relation_list = ['jealousy', 'envy', 'compassion', 'empathy', 'relationship','friendship', 'leadership','hostility', 'rejection', 'recognition',\n",
    "                 'rivalry', 'conformity', 'conflict', 'status', 'authority', 'legitimacy', 'popularity', 'disagreement', 'dissent',\n",
    "                 'interaction', 'communication', 'collaboration', 'coordination', 'cooperation', 'competition', 'conversation',\n",
    "                 'intimacy','responsibility']\n",
    "# change to human list not conducted in the current version of code\n",
    "human_list = ['parent', 'child', 'kid','sibling','brother','sister','mother','father',\n",
    "              'mom','dad','uncle','aunt','husband','wife','wives','spouse','fiance','fiancee','lover','friend', 'enemy','enemies',\n",
    "              'son','daughter','nephew','niece','cousin','neighbour','colleague','classmate','roommate']\n",
    "\n",
    "human_pub_list = ['professor','teacher','student','doctor','nurse','patient','priest',\n",
    "                  'rabbi','pastor','lawyer','officer','prisoner','inmate']\n",
    "\n",
    "total_ls = mind_list+body_list+relation_list+human_list+human_pub_list\n",
    "total_lemma_dict = lemmatize_words(total_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to all years\n",
    "files = os.listdir(data_path)\n",
    "selected_files = []\n",
    "for f in files:\n",
    "    id = int(f.split(\"_\")[1].split(\".\")[0])\n",
    "    if id>=81:\n",
    "        selected_files.append(f)\n",
    "selected_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d24203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_year_processing_pipeline(filename):\n",
    "    file_id = filename.split(\".\")[0]\n",
    "    df = txt_to_dataframe(data_path+filename)\n",
    "    # initial filtering\n",
    "    df['relevant_sent'] = df['speech'].apply(lambda x: sentence_extract(x,total_ls))\n",
    "    df = df[df['relevant_sent'].astype(bool)]\n",
    "    df = df.explode('relevant_sent')\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    # add formating: remove special punctuations other than normal ones\n",
    "    \n",
    "\n",
    "    # further filtering\n",
    "    df = parallelize_dataframe(df, apply_parallel, 1000, 8)\n",
    "    df = df_formatting(df)\n",
    "    df = df[df['if_selected']==True]\n",
    "    df.drop(columns=['relevant_sent', 'result','speech'], inplace=True)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    # to select sentences with higher quality\n",
    "    df['sent_length'] = df['sent_unmask'].apply(lambda x: sent_length(x))\n",
    "    df['noneng_ratio'] = df['sent_unmask'].apply(lambda x: ocr_quality_check(x))\n",
    "    df.to_pickle(processed_path+file_id+\"_obj_sents.pkl\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cad9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in selected_files[1:]:\n",
    "    single_year_processing_pipeline(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all dfs\n",
    "all_dfs = []\n",
    "for f in selected_files:\n",
    "    file_id = f.split(\".\")[0]\n",
    "    df = pd.read_pickle(processed_path+file_id+\"_obj_sents.pkl\")\n",
    "    all_dfs.append(df)\n",
    "all_df = pd.concat(all_dfs)\n",
    "all_df.reset_index(inplace=True, drop=True)\n",
    "all_df.to_pickle(processed_path + \"congress_total_obj_sents.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save object list for fill mask prediction\n",
    "all_objects = list(set(all_df['object']))\n",
    "# save\n",
    "with open(processed_path + \"human_nonhuman_masked_objects.pkl\", 'wb') as f:\n",
    "    pickle.dump(all_objects, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
